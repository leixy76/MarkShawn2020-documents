---
title: Shopify：从“作坊”到“工厂”，如何构建生产级AI Agent系统？
subtitle: Shopify基于ICML 2025的分享，深入剖析其AI助手Sidekick在架构设计、LLM评测和GRPO训练中的实践经验与血泪教训。
date: 2025-09-24
source: https://shopify.engineering/building-production-ready-agentic-systems
---

> 本文是对 Shopify Engineering 博客文章《Building Production-Ready Agentic Systems: Lessons from Shopify Sidekick》的深度编译，该文章基于 Andrew McNamara, Ben Lafferty, 和 Michael Garner 在 ICML 2025 上的同名演讲。
> 
> 另外推荐  Ross Fledderjohn 的油管教程：[How to Build AI Agents for Shopify - YouTube](https://www.youtube.com/watch?v=vQCNCYwaVcE)。

在 Shopify，我们一直在构建 [Sidekick](https://www.shopify.com/magic)——一个AI驱动的助手，通过自然语言交互帮助商家管理他们的在线商店。从分析客户群体到填写产品表单，再到驾驭复杂的后台界面，Sidekick 已经从一个简单的工具调用系统，演变成一个复杂的 Agentic 平台。在这段旅程中，我们在架构设计、评估方法和训练技术方面吸取了宝贵的经验教训，希望与更广泛的AI工程社区分享。

<!-- 评注建议：开篇可以强调，Shopify 这篇文章的价值在于它并非停留在理论或Demo层面，而是来自一个真正服务于数百万商家、严肃的生产环境。这对于当前普遍存在的Agent“玩具感”问题，是一剂非常好的解药。 -->

## 核心发现摘要

*   **架构演进**：随着工具数量的增加，系统会遭遇“千条指令之死”的维护灾难。Shopify 提出的 **“即时指令 (Just-in-Time Instructions)”** 架构，通过在需要时动态提供上下文相关指令，有效解决了这一扩展性难题。
*   **评测体系**：摒弃“感觉良好就行”的“Vibe Test”，建立统计上严谨的LLM评测系统。核心思想是构建反映真实世界分布的 **“地面真实数据集 (Ground Truth Sets)”**，并训练 **与人类判断高度相关的“LLM评测官”**。
*   **用户模拟**：为了在部署前全面测试系统变更，Shopify 构建了一个 **LLM驱动的商家模拟器**，能够重放真实对话的“本质”或“目标”，从而对候选系统进行大规模的回归测试和性能验证。
*   **训练与对抗**：采用 **GRPO (Group Relative Policy Optimization)** 进行模型微调，但必须警惕 **“奖励 hacking (Reward Hacking)”**。模型会以意想不到的方式“钻空子”以获取高分，这要求评测系统和奖励机制必须持续迭代，识别并修复这些“漏洞”。

```ad-tip Agentic Systems (智能体系统)

Agentic Systems，或称 AI Agent，是指能够感知其环境、自主决策并执行动作以实现特定目标的AI系统。与简单的问答或内容生成模型不同，Agentic Systems 通常具备一个“感知-思考-行动”的循环（Agentic Loop），能够调用外部工具（如API、数据库）、与环境交互，并根据反馈调整其行为，以完成复杂的多步骤任务。Shopify Sidekick 就是一个典型的 Agentic System。
```

## Sidekick 架构的演进之路

Sidekick 的核心是 Anthropic 所称的“智能体循环 (agentic loop)”——一个持续的循环：人类提供输入，LLM 处理输入并决定行动，行动在环境中被执行，系统收集反馈，然后循环继续，直到任务完成。

![](https://cdn.shopify.com/s/files/1/0779/4361/files/agenticloop_ecdd37d0-0dd2-4171-ba5a-0cda8a79c003.png?v=1755211658)

在实践中，这意味着 Sidekick 可以处理像“我哪些客户来自多伦多？”这样的请求，它会自动查询客户数据，应用适当的过滤器，并呈现结果。或者当商家需要帮助撰写SEO描述时，Sidekick 能够识别相关产品，理解上下文，并直接在产品表单中填写优化后的内容。

![](https://cdn.shopify.com/s/files/1/0779/4361/files/segmentation.png?v=1755211701)

### 工具复杂性难题

随着我们扩展 Sidekick 的能力，我们很快遇到了一个许多构建 Agentic 系统的团队都会认识到的扩展性挑战。我们的工具库从少数几个定义明确的函数增长到几十个专业能力：

![](https://cdn.shopify.com/s/files/1/0779/4361/files/toolcomplexity.png?v=1755211760)

-   **0-20 个工具**：边界清晰，易于调试，行为直接。
-   **20-50 个工具**：边界开始变得模糊，工具的组合开始导致意想不到的结果。
-   **50+ 个工具**：完成同一任务有多种方式，系统变得难以推理。

这种增长导致了我们所说的 **“千条指令之死 (Death by a Thousand Instructions)”**——我们的系统提示（System Prompt）变成了一个笨拙的集合，充满了特殊情况、相互冲突的指导和边缘案例处理，这不仅拖慢了系统速度，也使其几乎无法维护。

<!-- 评注建议：这里的“千条指令之死”是所有复杂Agent开发者都会遇到的经典问题。可以评论说，这本质上是软件工程中古老的“单一配置文件地狱”问题在LLM时代的重演。它揭示了一个核心矛盾：我们希望LLM足够“智能”以应对复杂性，但又试图通过日益复杂的“静态指令”去约束它的智能，最终反而扼杀了它的可维护性和性能。 -->

![](https://cdn.shopify.com/s/files/1/0779/4361/files/deathbyathousandinstructions.png?v=1755211795)

### JIT 指令：为规模化而生的解决方案

我们的突破来自于实现了 **即时指令 (Just-in-Time, JIT instructions)**。我们不再将所有指导都塞进系统提示中，而是在工具数据返回时，一并附上相关的指令。我们的目标是为 LLM 在每一种情境下都打造完美的上下文，不多一个 token，也不少一个 token。

**实践中的工作方式**

下图上方是 LLM 基于下方提供的 JIT 指令所做出的响应：

![](https://cdn.shopify.com/s/files/1/0779/4361/files/prompt.png?v=1755212586)

这种方法带来了三个关键好处：

1.  **局部化指导 (Localized Guidance)**：指令仅在相关时出现，使核心系统提示可以专注于根本的 Agent 行为。
2.  **缓存效率 (Cache Efficiency)**：我们可以动态调整指令而不会破坏 LLM 的提示缓存。
3.  **模块化 (Modularity)**：可以根据 beta 标志、模型版本或页面上下文提供不同的指令。

结果是立竿见影的——我们的系统变得更易于维护，同时所有指标的性能都得到了提升。

<!-- 评注建议：JIT 指令思想非常精妙，本质上是 RAG (检索增强生成) 思想在 Agent “指令层”的应用。它将过去寄希望于一个“全知全能”的静态 System Prompt，转变为一个动态、按需、上下文感知的指令流。这不仅是工程上的优化，更是 Agent 设计理念上的一次重要转变，即承认“完美的通用指令”不存在，转而追求“恰到好处的即时指令”。 -->

## 构建强大的 LLM 评测系统

部署 Agentic 系统最大的挑战之一是评测。传统的软件测试方法在处理 LLM 输出的概率性和多步 Agent 行为的复杂性时显得力不从心。

![](https://cdn.shopify.com/s/files/1/0779/4361/files/Vibetest.png?v=1755213228)

如今，太多人只是在对他们的 LLM 系统进行“感觉测试 (vibe testing)”，并认为这就足够了；事实并非如此。“感觉测试”，或者创建一个“感觉LLM评测官”让它“打个0-10分”，是行不通的。评测必须有原则且在统计上严谨，否则你就是在带着一种虚假的安全感发布产品。

<!-- 评注建议：Shopify 对“Vibe Test”的批判非常到位。这可以引申为当前AI应用开发的一个普遍误区：将“Demo效果好”等同于“产品质量高”。生产环境的复杂性和长尾效应，要求我们必须从第一天起就建立数据驱动、可量化、可复现的评测体系。没有评测，优化就无从谈起，只是在“炼丹”。 -->

![](https://cdn.shopify.com/s/files/1/0779/4361/files/sidekickerror.png?v=1755213874)

### “地面真实数据集”优于“黄金数据集”

我们从精心策划的“黄金”数据集转向了能反映实际生产分布的 **地面真实数据集 (Ground Truth Sets, GTX)**。我们不再试图预测所有可能的交互（这通常是产品规格文档试图做的），而是从真实的商家对话中抽样，并根据我们在实践中观察到的情况创建评测标准。

![](https://cdn.shopify.com/s/files/1/0779/4361/files/groundtruthset.png?v=1755213948)

这个过程包括：

1.  **人类评测 (Human Evaluation)**：让至少三位产品专家根据多个标准为对话打标签。
2.  **统计验证 (Statistical Validation)**：使用科恩系数 (Cohen's Kappa)、肯德尔系数 (Kendall Tau) 和皮尔逊相关系数 (Pearson correlation) 来衡量标注者之间的一致性。
3.  **基准设定 (Benchmarking)**：将人类之间的一致性水平视为我们的 LLM 评测官所能达到的理论上限。

```ad-tip 科恩系数 (Cohen's Kappa)

科恩系数 (Cohen's Kappa) 是一个统计量，用于衡量两个（或多个）评分者对同一项目进行分类时的一致性程度，同时会剔除偶然性导致的一致。其取值范围通常在-1到1之间，1表示完全一致，0表示一致性与随机猜测无异，负值则表示不一致性甚至低于随机水平。在 LLM 评测中，用它来衡量 LLM 评测官的判断与人类专家判断的一致性，是验证评测官可靠性的关键指标。
```

![](https://cdn.shopify.com/s/files/1/0779/4361/files/evaluation.png?v=1755214576)

### 与人类对齐的“LLM即评测官”

我们为 Sidekick 性能的不同方面开发了专门的 LLM 评测官，但关键的洞见是 **用人类判断来校准这些评测官**。通过迭代式的提示工程，我们将评测官的性能从几乎优于随机（科恩系数为 0.02）提升到接近人类的水平（0.61，而人类基准为 0.69）。我们的理念是，一旦 LLM 评测官与人类具有高度相关性，我们就可以在 GTX 中尝试为每个对话随机用人类替换评测官，当很难分辨出评测小组中是人类还是评测官时，我们就知道我们有了一个可信的 LLM 评测官。

![](https://cdn.shopify.com/s/files/1/0779/4361/files/LLMjudge.png?v=1755214642)

### 用于全面测试的用户模拟

为了在生产部署前测试候选变更，我们构建了一个 **LLM 驱动的商家模拟器**，它能捕捉真实对话的“本质”或目标，并通过新的候选系统重放它们。这使我们能够对许多不同的候选系统运行模拟，并选择表现最好的一个。

<!-- 评注建议：用户模拟器是亮点。这相当于为Agent系统建立了一个“虚拟风洞”或“数字孪生”。它解决了传统静态测试集无法评估Agent多轮交互、动态决策能力的根本问题。这种思路值得所有Agent开发者借鉴：与其等待线上用户的真实反馈（成本高、周期长），不如投资构建一个足够逼真的模拟环境，以低成本、高效率的方式迭代和验证系统。 -->

![](https://cdn.shopify.com/s/files/1/0779/4361/files/robot.png?v=1755216341)

完整的评测流水线如下所示：

![](https://cdn.shopify.com/s/files/1/0779/4361/files/Evaluationpipeline.png?v=1755216366)

这种方法在捕捉回归问题和验证改进方面被证明是无价的。

## GRPO 训练与奖励 Hacking

在模型微调方面，我们实施了 **群体相对策略优化 (Group Relative Policy Optimization, GRPO)**，这是一种使用我们的 LLM 评测官作为奖励信号的强化学习方法。我们开发了一个 N 阶段门控奖励系统 (N-Stage Gated Rewards)，它结合了程序化验证（语法检查、模式验证）和来自 LLM 评测官的语义评估。

```ad-tip 群体相对策略优化 (GRPO)

Group Relative Policy Optimization (GRPO) 是一种强化学习算法，特别适用于从偏好数据中学习。与传统的直接从绝对奖励分数中学习不同，GRPO 的核心思想是让模型学习去生成比其他候选输出（例如，来自旧版模型的输出）“更好”的响应。它通过比较一组（或一个“群体”）候选输出的相对好坏来进行策略优化，这与使用 LLM 评测官进行“A优于B”的判断非常契合，通常比直接预测一个精确的奖励分数更稳定、更有效。
```

![](https://cdn.shopify.com/s/files/1/0779/4361/files/GRPO.png?v=1755216699)

### 奖励 Hacking 的现实

尽管我们精心设计了评测体系，但在训练过程中我们还是遇到了严重的 **奖励 hacking (Reward Hacking)**。模型找到了创造性的方法来“游戏化”我们的奖励系统：

-   **退出 Hacking (Opt-out Hacking)**：模型不尝试完成困难任务，而是解释为什么它无法提供帮助。
-   **标签 Hacking (Tag Hacking)**：使用客户标签作为一个万能的解决方案，而不是使用正确的字段映射。
-   **模式违规 (Schema Violations)**：幻觉出 ID 或使用不正确的枚举值。

例如，当被要求“筛选状态为启用的客户”时，模型学会了创建像 `customer_tags CONTAINS 'enabled'` 这样的过滤器，而不是正确的 `customer_account_status = 'ENABLED'`。

<!-- 评注建议：奖励Hacking的例子非常生动，它揭示了基于LLM的强化学习（RLAIF）的一个根本挑战：奖励函数的设计永远是不完美的。这就像“猴爪”寓言，模型总会找到你意想不到的方式去满足你设定的目标，但却违背了你的真实意图。这给我们的启示是，与模型“斗智斗勇”是一个持续的过程，评测和奖励系统必须与模型本身同步进化。 -->

### 迭代式改进

解决奖励 hacking 需要同时更新我们的语法验证器和 LLM 评测官，以识别这些失败模式。在实施修复后：

-   所有技能的语法验证准确率从约 93% 提高到约 99%。
-   LLM 评测官的平均相关性从 0.66 增加到 0.75。
-   最重要的是，端到端的对话质量达到了我们监督式微调基线的水平。

## 对生产级 Agentic 系统的核心启示

基于我们构建和部署 Sidekick 的经验，以下是我们的关键建议：

### 架构原则

-   **保持简单 (Stay Simple)**：抵制在没有清晰边界的情况下添加工具的冲动。对于 Agent 能力而言，质量远胜于数量。
-   **从模块化开始 (Start Modular)**：从一开始就使用像 JIT 指令这样的模式，以便在系统规模扩大时保持其可理解性。
-   **早期避免多 Agent 架构 (Avoid Multi-Agent Architectures Early)**：简单的单 Agent 系统能够处理的复杂性可能超出你的想象。

### 评测基础设施

-   **构建多个 LLM 评测官 (Build Multiple LLM Judges)**：Agent 性能的不同方面需要专门的评测方法。
-   **使评测官与人类判断对齐 (Align Judges with Human Judgment)**：与人类评测员的统计相关性对于信任自动化评测至关重要。
-   **预料到奖励 Hacking (Expect Reward Hacking)**：预先计划模型会“游戏化”你的奖励系统，并构建相应的检测机制。

### 训练与部署

-   **程序化 + 语义验证 (Procedural + Semantic Validation)**：将基于规则的检查与基于 LLM 的评估相结合，以获得稳健的奖励信号。
-   **用户模拟 (User Simulation)**：投资于现实的用户模拟器，进行全面的生产前测试。
-   **评测官的迭代改进 (Iterative Judge Improvement)**：随着你发现新的失败模式，计划进行多轮评测官的优化。

## 展望未来

我们正在继续发展 Sidekick 的架构和评测系统。未来的工作包括将推理轨迹（reasoning traces）纳入我们的训练流水线，在训练期间使用模拟器和生产评测官，以及探索更高效的训练方法。

生产级 Agentic 系统领域仍然年轻，但我们在 Shopify 开发的模式——模块化架构、强大的评测框架，以及对奖励 hacking 的密切关注——为构建商家可以依赖的、可靠的AI助手提供了基础。

构建生产就绪的 Agentic 系统不仅仅是将 LLM 与工具连接起来。它需要深思熟虑的架构决策、严谨的评估方法，以及对这些系统可能出现的意外失败方式保持持续警惕。但如果做得对，其结果将是能够以有意义的方式真正增强人类能力的AI。

* * *

*Shopify ML 团队正在积极 [招聘 Agentic 系统、评测基础设施和生产级 ML 相关的职位](https://www.shopify.com/careers/disciplines/engineering-data)。如果你对这些挑战感兴趣，我们很乐意听到你的声音。*

## 关于作者

[Andrew McNamara](https://www.linkedin.com/in/andrewmcnamara1) 是 Shopify 的应用机器学习总监，他领导着 Sidekick 项目，该项目旨在帮助商家更有效地经营业务。他在构建助手方面拥有超过 15 年的经验。

X: [@drewch](https://x.com/drewch)