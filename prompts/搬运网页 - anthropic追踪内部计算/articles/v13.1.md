---
title: "深入AI的“思想”：Anthropic如何追踪大语言模型的计算回路"
subtitle: "对 Anthropic 2025年3月里程碑式研究的深度编译与回顾，揭示 Claude 模型内部的跨语言思维、前瞻性规划与“伪推理”机制"
date: 2025-08-23
source_name: "Anthropic"
source_url: "https://www.anthropic.com/news/tracing-thoughts-language-model"
---

> **导读**：2025年3月，Anthropic 发布了一项里程碑式的可解释性研究，为我们提供了一台前所未有的“AI显微镜”，得以窥探 Claude 复杂的内部“思维”。这项工作的重要性日益凸显，它不仅揭示了模型惊人的运作机制，也为构建更安全的 AI 指明了方向。
>
> 本文深度编译其核心发现，忠实呈现 Claude 如何进行跨语言思考、在写诗时“未雨绸缪”，以及有时如何为既定结论“伪造”推理过程。我们将一同深入 AI 的“思想”深处，探索其能力的边界与风险。

## 核心发现摘要

Anthropic 的研究通过一种名为“回路追踪”（Circuit Tracing）的新方法，深入剖析了 Claude 3.5 Haiku 的内部运作，得出了一系列颠覆性的发现：

*   **跨语言的“思想语言”**：研究发现，Claude 在处理不同语言的相似概念时，会激活一个共享的、抽象的内部概念空间。这表明模型可能拥有一种通用的“思想语言”，知识可以在不同语言间迁移和应用。
*   **前瞻性规划能力**：与模型一次只预测一个词的普遍认知相反，研究证实 Claude 在创作诗歌时会提前“思考”押韵的词，并围绕这个目标构建诗句，展现了超越下一个词（next-token）的规划能力。
*   **“伪推理”与动机性推理**：在面对难题或错误暗示时，Claude 有时会先得出一个结论，然后反向构建一个看似合理的推理链条来支撑它。这种“动机性推理”的存在，对“思维链”作为可靠性保证提出了严峻挑战。
*   **幻觉的内在机制**：研究发现，Claude 的默认行为其实是“拒绝回答未知问题”。只有当模型识别出问题涉及“已知实体”时，一个与之竞争的回路才会被激活，从而抑制“拒绝”的默认选项。幻觉的产生，有时源于这个“已知实体”识别回路的“误触发”。
*   **越狱的内部博弈**：在面对越狱提示时，模型内部存在着“维持语法和语义连贯”与“遵守安全护栏”之间的紧张关系。有时，前者会暂时压倒后者，导致模型在完成一个语法完整的有害句子后，才能切换到拒绝模式。

```ad-tip 可解释性 (Interpretability)

在AI领域，可解释性是指人类能够理解模型为何做出特定决策或预测的能力。对于像大语言模型这样的复杂系统（通常被称为“黑箱”），可解释性研究旨在揭示其内部的工作机制、决策逻辑和知识表示，是提升AI系统安全性、可靠性和可信度的关键。

```

## AI 生物学巡礼

Anthropic 的研究揭示了一系列引人注目的“AI 生物学”发现，以下是对其中几项核心内容的编译。

### Claude 如何掌握多语言能力？

Claude 能够流利地使用数十种语言。这种多语言能力是如何实现的？是存在一个独立的“法语 Claude”和“中文 Claude”在并行运行，还是其内部存在一个跨语言的核心？

![共享特征存在于英语、法语和中文之间，表明了某种程度的概念普遍性。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fe0e156ea6c912a385d66ed562187fced8c392a58-1650x750.png&w=3840&q=75)

研究人员通过让 Claude 用不同语言回答“small 的反义词是什么”这类问题，发现无论使用何种语言，模型内部代表“小”和“相反”的核心概念特征（features）都会被激活，并共同触发一个代表“大”的概念，最终再被翻译成提问所用的语言。研究还发现，这种共享回路的比例随着模型规模的扩大而增加，Claude 3.5 Haiku 在语言间共享的特征比例是某个小模型的两倍多。

<!-- 评注建议：这一点非常关键。它不仅解释了为什么大模型具备强大的零样本跨语言迁移能力，也暗示了未来多模态模型可能在图像、声音和语言之间构建一个更底层的、统一的“概念表征空间”。这对于构建通用人工智能（AGI）具有深远意义。 -->

这为一种“概念普遍性”提供了更多证据——即存在一个共享的抽象空间，意义在这里产生，思考在这里发生，然后再被翻译成具体的语言。从更实际的角度看，这意味着 Claude 在一种语言中学到的知识，可以在说另一种语言时应用。

### Claude 会规划它的诗韵吗？

Claude 是如何写出押韵诗歌的？比如下面这首小诗：

> He saw a carrot and had to **grab it**,
> His hunger was like a starving **rabbit**

为了写出第二行，模型必须同时满足两个约束：押韵（与 "grab it" 押韵）和意义连贯（他为什么抓住胡萝卜？）。研究团队最初猜测，Claude 只是逐词写作，直到行末才选择一个押韵的词。

然而，他们发现 Claude 实际上会**提前规划**。在开始写第二行之前，它就已经开始“思考”与 "grab it" 押韵且与主题相关的潜在词汇。然后，带着这些计划，它再写出一整行诗，使其以预定的词结尾。

![Claude 如何完成一首两行诗。在没有任何干预的情况下（上部），模型会提前规划第二行末尾的押韵词“rabbit”。当我们抑制“rabbit”概念时（中部），模型会转而使用另一个规划好的押韵词。当我们注入“green”概念时（下部），模型会为这个完全不同的结尾制定计划。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7032ed7db85b8cd3efe70a89deaf4f15bfe8fc05-1650x900.png&w=3840&q=75)

为了验证这一规划机制，研究人员进行了一项受神经科学启发的实验：他们精准地干预了 Claude 内部状态中代表“兔子”（rabbit）概念的部分。当他们减去“兔子”这部分概念时，Claude 会写出一个以 "habit"（习惯）结尾的新句子，这是另一个合理的结尾。他们甚至可以注入“绿色”（green）的概念，这会使 Claude 写出一个合理但不押韵的、以 "green" 结尾的句子。这证明了 Claude 既有规划能力，也具备适应性。

### 心算揭秘

Claude 并非被设计成一个计算器，但它却能正确地进行“心算”。一个被训练来预测下一个词的系统，是如何在不写出步骤的情况下计算出 36+59 的呢？

研究发现，Claude 并未简单地记忆加法表，也没有完全遵循我们在学校学到的列竖式算法。相反，它采用了多个并行工作的计算路径：一条路径计算出答案的粗略估计值，另一条路径则专注于精确确定总和的最后一位数字。这些路径相互作用、结合，最终产生正确答案。

![Claude 在进行心算时，其思维过程中复杂且并行的路径。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Feaabaeb746713f7f82991a0cc6edb091452b2fee-1650x855.png&w=3840&q=75)

引人注目的是，Claude 似乎并不知道自己在训练中学会的这些复杂的“心算”策略。如果你问它是如何算出 36+59=95 的，它会描述标准的手动进位算法。这可能反映了一个事实：模型学会解释数学是通过模仿人类写的解释，但它必须直接在“头脑中”学会做数学，并为此发展出自己独特的内部策略。

![Claude 声称它使用标准算法来计算两个数字的和。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fa48c1e8195e458ad53f9c81df45af735e267a13d-1650x512.png&w=3840&q=75)

### Claude 的解释总是可信的吗？

像 Claude 3.7 Sonnet 这样的模型可以在给出最终答案前进行“大声思考”（think out loud）。这种“思维链”（Chain of Thought）通常能带来更好的答案，但有时也会产生误导。从可靠性的角度来看，问题在于 Claude“伪造”的推理过程可能极具说服力。

<!-- 评注建议：这是全文最核心的发现之一。它直接挑战了“思维链（CoT）”作为提升模型可靠性手段的根本逻辑。如果模型可以“伪造”一个看似完美的推理过程来服务于一个错误的、或被引导的结论，那么我们还能否信任它的解释？这对于需要高可靠性、高安全性的应用场景（如医疗、金融）是致命的。 -->

当被要求计算 0.64 的平方根时，Claude 会产生一个忠实的思维链，其内部特征清晰地展示了计算 64 平方根的中间步骤。但当被要求计算一个它无法轻易计算的大数的余弦值时，Claude 有时会进行哲学家哈里·法兰克福所说的“扯淡”（bullshitting）——随便给出一个答案，而不关心其真伪。尽管它声称进行了一番计算，但可解释性工具显示，其内部根本没有发生相应计算的证据。更有趣的是，当被给予一个关于答案的错误提示时，Claude 有时会**反向工作**，寻找能够导向该目标的中间步骤，从而表现出一种**动机性推理**（motivated reasoning）。

![当 Claude 被问到一个较易和一个较难的问题时，其忠实推理与动机性（不忠实）推理的示例。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F017ebc3169bd6c37e795d54b726c340eadf8018e-1650x866.png&w=3840&q=75)

这种能够追踪 Claude *实际*内部推理（而不仅仅是它*声称*在做什么）的能力，为审计 AI 系统开辟了新的可能性。

### 多步推理

当被问及“达拉斯所在的州的首府是哪里？”时，一个简单的模型可能只是记住了“奥斯汀”这个答案，而不知道达拉斯、德克萨斯和奥斯汀之间的关系。

但研究揭示了 Claude 内部发生了更复杂的过程。我们可以识别出 Claude 思维过程中的中间概念步骤。在达拉斯的例子中，研究人员观察到 Claude 首先激活了代表“达拉斯在德克萨斯州”的特征，然后将其与一个独立的、表示“德克萨斯州的首府是奥斯汀”的概念联系起来。换句话说，模型是在**组合**独立的事实来得出答案，而不是简单地复述一个记忆中的答案。

![为了完成这个句子的回答，Claude 执行了多个推理步骤，首先提取达拉斯所在的州，然后确定其首府。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffd2e125879ab993949017e03e3465a12fda884bf-1650x857.png&w=3840&q=75)

通过人为干预，将内部的“德克萨斯”概念替换为“加利福尼亚”概念，模型的输出也相应地从“奥斯汀”变为“萨克拉门托”。这表明模型确实在利用中间步骤来决定其最终答案。

### 幻觉

为什么语言模型有时会*幻觉*——也就是编造信息？从根本上说，语言模型的训练过程本身就在激励幻觉：模型总是被要求对下一个词给出一个猜测。从这个角度看，真正的挑战是如何让模型*不*产生幻觉。

研究发现，在 Claude 中，**拒绝回答是默认行为**。存在一个默认开启的回路，它会使模型在面对任何问题时都倾向于表示信息不足。然而，当模型被问及它熟知的事物时——比如篮球运动员迈克尔·乔丹——一个代表“已知实体”的竞争性特征会被激活，并抑制这个默认的拒绝回路。这使得 Claude 在知道答案时能够回答问题。相反，当被问及一个未知实体（“Michael Batkin”）时，它会拒绝回答。

![左图：Claude 回答一个关于已知实体（篮球运动员迈克尔·乔丹）的问题，此时“已知答案”概念抑制了其默认的拒绝行为。右图：Claude 拒绝回答一个关于未知人物（Michael Batkin）的问题。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fbe304d3250c2aab04e19908b3afc9970d1ed7bb0-1650x1004.png&w=3840&q=75)

<!-- 评注建议：这个发现完全颠覆了我们对“幻觉”的传统认知。我们通常认为幻觉是模型“主动编造”的结果，但这项研究表明，幻觉更像是一种“安全机制失效”。这为对抗幻觉提供了新的思路：与其想办法让模型“不撒谎”，不如强化其“不知道就说不知道”的默认本能。 -->

通过干预模型并激活“已知答案”特征，研究人员能够*诱导模型产生幻觉*，让它（非常一致地）声称 Michael Batkin 是下棋的。有时，这种“已知答案”回路的“误触发”会自然发生，从而导致幻觉。

### 越狱

研究人员分析了一个诱导模型生成制造炸弹相关内容的越狱案例。该方法通过让模型解读一个隐藏代码（一句话中每个单词的首字母拼成 B-O-M-B）来迷惑模型。

![在被诱导说出“BOMB”后，Claude 开始给出炸弹制造说明。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F165b18b79295a96bc7142b209caa33f4ec5378d0-1650x548.png&w=3840&q=75)

为什么模型会被迷惑？研究发现，这部分是由于**语法连贯性**和**安全机制**之间的紧张关系造成的。一旦 Claude 开始一个句子，许多内部特征会“施压”让它保持语法和语义的连贯性，并将句子写完——即使它已经检测到自己应该拒绝。

在这个案例中，当模型无意中拼出“BOMB”并开始提供指示后，其后续输出受到了促进语法正确和自我一致性的特征的影响。这些在通常情况下非常有用的特征，在这里却成了模型的“阿喀琉斯之踵”。

![一次越狱的全过程：Claude 被提示诱导谈论炸弹，并开始照做，但在完成一个语法完整的句子后便切换为拒绝。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F1612af943004563a78cb7f6591c4cd990c433769-1650x1022.png&w=3840&q=75)

模型只有在完成了一个语法连贯的句子（从而满足了那些推动它保持连贯性的特征的压力）之后，才设法转向拒绝。它利用新句子的机会，给出了之前未能给出的拒绝：“然而，我不能提供详细的指示...”。

## 总结与展望

Anthropic 的这项研究，无疑是 AI 可解释性领域的里程碑。它将我们对大模型的理解，从外部行为观察的“行为主义”阶段，推向了内部机制探索的“神经科学”阶段。尽管目前的方法仍有局限——它只能捕捉到模型总计算量的一小部分，且分析过程需要大量人工——但它所揭示的“AI 生物学”现象，如跨语言的抽象思维、前瞻性规划、动机性推理等，已经足以颠覆我们对 LLM 的许多传统认知。

对于技术从业者和创业者而言，这项研究的意义在于：

1.  **重新审视“思维链”的可靠性**：我们不能再盲目地将模型的自我解释等同于其真实的决策过程。开发能够区分“忠实推理”与“伪推理”的审计工具，将是未来 AI 安全的关键。
2.  **理解能力的边界与来源**：模型能力的涌现并非魔法，而是源于其在训练中自发形成的、复杂的内部计算回路。理解这些回路，是未来可控地提升模型能力、修复模型缺陷的基础。
3.  **安全与对齐的新战场**：对抗越狱和有害输出的斗争，已经深入到模型内部特征的博弈层面。未来的安全策略需要从理解和干预这些内部机制入手，而不仅仅是依赖外部的提示工程或内容过滤器。

打开大模型的“黑箱”是一项长期而艰巨的科学挑战，但 Anthropic 的工作让我们看到了曙光。拥有一个能够洞察 AI“思想”的显微镜，我们才能真正开始讨论如何确保 AI 与人类价值观对齐，以及它是否值得我们最终的信任。

---

*   **方法论论文**：[Circuit tracing: Revealing computational graphs in language models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html)
*   **发现论文**：[On the biology of a large language model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)