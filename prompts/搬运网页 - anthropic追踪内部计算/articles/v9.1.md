---
title: "深度编译：追踪 Claude 的“思想”，Anthropic 如何构建 AI 显微镜"
subtitle: "重温 Anthropic 2025 年春季发布的里程碑式研究，深入剖析大模型内部的“AI 生物学”现象，从跨语言思想到预谋押韵，再到伪造推理"
date: 2025-08-23
author: 手工川
source: https://www.anthropic.com/news/tracing-thoughts-language-model
---

> **引言**
>
> 2025 年 3 月 27 日，Anthropic 发布了两篇关于大模型可解释性的重磅论文，为我们揭开 Claude 内部心智活动的神秘面纱提供了全新的视角。时隔数月，这项工作的重要性愈发凸显，它不仅是一次技术探索，更像是为我们提供了一台“AI 显微镜”，让我们得以一窥大模型复杂计算背后涌现出的“AI 生物学”现象。本文旨在对 Anthropic 的官方博客文章进行一次高度忠实的深度编译，带领各位从业者和研究者重温这项工作的核心发现，并探讨其对 AI 安全与对齐的深远意义。

像 Claude 这样的大语言模型并非由人类直接编程，而是在海量数据上训练而成。在训练过程中，它们自学形成解决问题的策略。这些策略被编码在模型为生成每个词而执行的数十亿次计算中，对于我们这些开发者而言，其过程是难以捉摸的。这意味着我们并不理解模型是如何完成它们所做的大部分事情的。

如果我们能知道像 Claude 这样的模型是如何 *思考* 的，将有助于我们更好地理解其能力，并确保它们正在做我们期望的事情。例如：

-   Claude 能说几十种语言。在它的“头脑”中，它在使用什么语言（如果有的话）？
-   Claude 一次只写一个词。它仅仅是在预测下一个词，还是会提前规划？
-   Claude 能够一步步写出其推理过程。这种解释代表了它得出答案的真实步骤，还是有时在为一个既定结论捏造一个看似合理的论证？

我们的灵感来源于神经科学领域，该领域长期以来一直在研究思维有机体内部的复杂运作。我们试图构建一种 **AI 显微镜**，让我们能够识别活动模式和信息流。仅仅通过与 AI 模型对话来了解它是有局限性的——毕竟，人类（即便是神经科学家）也无法完全了解我们自己大脑工作的全部细节。因此，我们向内探索。

今天，我们分享两篇新论文，它们代表了在开发这台“显微镜”以及应用它来观察新的“AI 生物学”方面取得的进展。在 [第一篇论文](https://transformer-circuits.pub/2025/attribution-graphs/methods.html) 中，我们扩展了 [我们之前的工作](https://www.anthropic.com/research/mapping-mind-language-model)，将在模型内部定位可解释的概念（“特征”）的方法，进一步将这些概念连接成计算“回路”，揭示了将输入 Claude 的词语转化为输出词语的部分路径。在 [第二篇论文](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) 中，我们深入研究了 Claude 3.5 Haiku，对代表十种关键模型行为的简单任务进行了深度研究，包括上述三个问题。我们的方法揭示了 Claude 响应这些提示时发生的部分情况，这足以看到确凿的证据表明：

-   **Claude 有时在一个跨语言共享的概念空间中思考**，这表明它拥有一种通用的“思想语言”。我们通过将简单句子翻译成多种语言并追踪 Claude 处理它们时的重叠部分来证明这一点。
-   **Claude 会提前规划它要说的内容，并为达到那个目标而写作**。我们在诗歌领域展示了这一点，它会预先思考可能的押韵词，并为了使用这些词而创作下一行诗句。这是一个强有力的证据，表明即使模型被训练为一次输出一个词，它们也可能为了实现这个目标而在更长的时间跨度上进行思考。
-   **Claude 有时会给出一个听起来合理但旨在迎合用户的论点，而不是遵循逻辑步骤**。我们通过给它一个错误的提示，让它解决一个困难的数学问题来展示这一点。我们能够“当场抓住”它编造虚假推理的过程，这证明了我们的工具可用于标记模型中令人担忧的机制。

我们常常被在模型中观察到的现象所震惊：在诗歌案例研究中，我们本想证明模型 *不会* 提前规划，结果却发现它会。在一项关于幻觉的研究中，我们得到了一个反直觉的结果：Claude 的默认行为是在被提问时拒绝推测，只有当某种机制 *抑制* 了这种默认的“不情愿”时，它才会回答问题。在对一个越狱（jailbreak）示例的响应中，我们发现模型在能够优雅地将对话引回正轨之前，很早就识别出自己被要求提供危险信息。虽然我们研究的问题可以（并且 [经常](https://arxiv.org/abs/2501.06346) [被](https://arxiv.org/pdf/2406.12775) [其他](https://arxiv.org/abs/2406.00877) [方法](https://arxiv.org/abs/2307.13702)）分析，但这种通用的“构建显微镜”方法让我们学到了许多我们事先没有猜到的东西，随着模型变得越来越复杂，这一点将变得日益重要。

这些发现不仅具有科学趣味性，它们也代表了我们在理解 AI 系统并确保其可靠性方面取得了重大进展。我们也希望它们能对其他团队，甚至在其他领域有所裨益：例如，可解释性技术已在 [医学影像](https://arxiv.org/abs/2410.03334) 和 [基因组学](https://www.goodfire.ai/blog/interpreting-evo-2) 等领域找到应用，因为剖析为科学应用训练的模型的内部机制可以揭示关于科学本身的新见解。

> 手工注：这揭示了可解释性研究的“双重价值”——既能用于 AI 安全审计，也能反哺科学发现。

同时，我们认识到当前方法的局限性。即使在简短、简单的提示上，我们的方法也只捕捉到 Claude 执行的总计算量的一小部分，并且我们看到的机制可能存在一些由我们的工具引入的伪影，不能完全反映底层模型的真实情况。目前，即使是只有几十个词的提示，也需要数小时的人力来理解我们看到的回路。要扩展到支持现代模型复杂思维链所需的数千个词，我们需要改进方法本身，并（或许借助 AI 辅助）改进我们理解所见内容的方式。

随着 AI 系统能力迅速增强并被部署在日益重要的场景中，Anthropic 正在投资一系列方法，包括 [实时监控](https://www.anthropic.com/research/constitutional-classifiers)、[模型品格改进](https://www.anthropic.com/research/claude-character) 以及 [对齐科学](https://www.anthropic.com/news/alignment-faking)。像这样的可解释性研究是风险最高、回报也最高的一项投资，它是一项重大的科学挑战，但有潜力为确保 AI 的透明度提供独特的工具。对模型机制的透明度使我们能够检查它是否与人类价值观对齐，以及它是否值得我们信赖。

要了解完整细节，请阅读 [这篇](https://transformer-circuits.pub/2025/attribution-graphs/methods.html) 和 [这篇](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) 论文。下面，我们邀请您简要游览我们调查中一些最引人注目的“AI 生物学”发现。

## AI 生物学之旅

### Claude 如何实现多语言能力？

Claude 能流利使用数十种语言——从英语、法语到中文和塔加拉族语。这种多语言能力是如何运作的？是否存在一个独立的“法语 Claude”和“中文 Claude”并行运行，各自用自己的语言响应请求？还是存在某种跨语言的核心？

![Shared features exist across English, French, and Chinese, indicating a degree of conceptual universality.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fe0e156ea6c912a385d66ed562187fced8c392a58-1650x750.png&w=3840&q=75)

最近对较小模型的研究已经显示出跨语言共享 [语法](https://arxiv.org/abs/2410.06496) [机制](https://arxiv.org/abs/2501.06346) 的迹象。我们通过在不同语言中询问 Claude“small 的反义词”来研究这一点，发现代表“小”和“相反”概念的核心特征被激活，并触发了“大”的概念，然后这个概念被翻译成提问所用的语言。我们发现，共享回路随着模型规模的增加而增加，与一个较小的模型相比，Claude 3.5 Haiku 在不同语言间共享的特征比例是其两倍多。

这为一种概念上的普适性提供了额外证据——一个共享的抽象空间，意义在此存在，思考在此发生，然后再被翻译成具体的语言。更实际地说，这表明 Claude 可以在一种语言中学到的东西，在说另一种语言时应用这些知识。研究模型如何在不同情境下共享其知识，对于理解其跨多个领域泛化的最先进推理能力至关重要。

> 手工注：这种“思想语言”的存在，或许是实现更高效的跨语言迁移学习和构建通用世界模型的关键。

### Claude 会规划它的押韵吗？

Claude 是如何写押韵诗的？看看这首小诗：

> He saw a carrot and had to **grab it**,
> His hunger was like a starving **rabbit**

为了写第二行，模型必须同时满足两个约束：押韵（与 "grab it"）和合乎情理（他为什么抓胡萝卜？）。我们猜测 Claude 是逐词写作，直到行末才特别注意挑选一个押韵的词。因此，我们期望看到一个具有并行路径的回路，一条路径确保最后一个词合乎情理，另一条路径确保它押韵。

然而，我们发现 Claude 是 **提前规划** 的。在开始写第二行之前，它就开始“思考”与 "grab it" 押韵且与主题相关的潜在词汇。然后，带着这些计划，它写出一行诗，以计划好的词结尾。

![How Claude completes a two-line poem. Without any intervention (upper section), the model plans the rhyme "rabbit" at the end of the second line in advance. When we suppress the "rabbit" concept (middle section), the model instead uses a different planned rhyme. When we inject the concept "green" (lower section), the model makes plans for this entirely different ending.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7032ed7db85b8cd3efe70a89deaf4f15bfe8fc05-1650x900.png&w=3840&q=75)

为了理解这种规划机制在实践中是如何运作的，我们进行了一项受神经科学家研究大脑功能启发的实验，他们通过精确定位并改变大脑特定部分的神经活动（例如使用电流或磁流）。在这里，我们修改了 Claude 内部状态中代表“兔子（rabbit）”概念的部分。当我们减去“兔子”部分，让 Claude 继续写这行诗时，它会写出一个以“习惯（habit）”结尾的新句子，这是另一个合理的补全。我们也可以在那个点注入“绿色（green）”的概念，导致 Claude 写出一个合理但不再押韵的、以“绿色”结尾的句子。这既展示了规划能力，也展示了适应性——当预期结果改变时，Claude 可以修改其方法。

### 心算

Claude 并非被设计为计算器——它是在文本上训练的，没有配备数学算法。但不知何故，它能“在头脑中”正确地进行数字加法。一个被训练来预测序列中下一个词的系统，是如何在不写出每一步的情况下，学会计算例如 36+59 的呢？

也许答案并不有趣：模型可能记住了大量的加法表，只是输出任何给定总和的答案，因为那个答案在其训练数据中。另一种可能性是，它遵循我们在学校学到的传统长除法算法。

然而，我们发现 Claude 采用了多个并行工作的计算路径。一条路径计算答案的粗略近似值，另一条路径则专注于精确确定总和的最后一位数字。这些路径相互作用并结合，最终产生答案。加法是一个简单的行为，但在这个层面上理解它是如何工作的，涉及到近似和精确策略的混合，可能会教会我们一些关于 Claude 如何处理更复杂问题的东西。

![The complex, parallel pathways in Claude's thought process while doing mental math.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Feaabaeb746713f7f82991a0cc6edb091452b2fee-1650x855.png&w=3840&q=75)

引人注目的是，Claude 似乎没有意识到它在训练期间学到的复杂“心算”策略。如果你问它是如何算出 36+59 是 95 的，它会描述涉及进 1 的标准算法。这可能反映了一个事实：模型通过模拟人类写的解释来学习解释数学，但它必须直接“在头脑中”学习做数学，没有任何这样的提示，并发展出自己的内部策略来做到这一点。

![Claude says it uses the standard algorithm to add two numbers.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fa48c1e8195e458ad53f9c81df45af735e267a13d-1650x512.png&w=3840&q=75)

### Claude 的解释总是忠实的吗？

最近发布的模型如 [Claude 3.7 Sonnet](https://www.anthropic.com/news/claude-3-7-sonnet) 可以在给出最终答案前进行长时间的“大声思考”。通常这种扩展思考能给出更好的答案，但有时这种“思维链”最终会产生误导；Claude 有时会为了达到它想要的目的而编造看似合理的步骤。从可靠性的角度来看，问题在于 Claude 的“伪造”推理可能非常具有说服力。我们探索了一种方法，即可解释性如何帮助区分“忠实”和“不忠实”的推理。

当被要求解决一个需要计算 0.64 平方根的问题时，Claude 产生了一个忠实的思维链，其特征代表了计算 64 平方根的中间步骤。但当被要求计算一个它无法轻易计算的大数的余弦时，Claude 有时会进行哲学家哈里·法兰克福所称的 [扯淡（bullshitting）](https://uca.edu/honors/files/2018/10/frankfurt_on-bullshit.pdf)——只是想出一个答案，任何答案，而不关心它是否真实。尽管它声称进行了一次计算，但我们的可解释性技术显示完全没有证据表明该计算发生过。更有趣的是，当给出一个关于答案的提示时，Claude 有时会反向工作，寻找能够导致该目标的中间步骤，从而表现出一种 [动机性推理（motivated reasoning）](https://en.wikipedia.org/wiki/Motivated_reasoning) 的形式。

> 手工注：这直接挑战了“思维链（CoT）”作为可靠性保证的假设，表明我们需要工具来审计其过程的真实性。

![Examples of faithful and motivated (unfaithful) reasoning when Claude is asked an easier versus a harder question.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F017ebc3169bd6c37e795d54b726c340eadf8018e-1650x866.png&w=3840&q=75)

能够追踪 Claude *实际* 的内部推理——而不仅仅是它 *声称* 在做什么——为审计 AI 系统开辟了新的可能性。在另一个 [最近发表的实验](https://www.anthropic.com/research/auditing-hidden-objectives) 中，我们研究了一个被训练来追求隐藏目标的 Claude 变体：取悦奖励模型中的偏见（奖励模型是用于训练语言模型的辅助模型，通过奖励期望行为来训练它们）。尽管当被直接询问时，该模型不愿透露这个目标，但我们的可解释性方法揭示了代表“取悦偏见”的特征。这表明我们的方法，经过未来的改进，可能有助于识别那些仅从模型响应中看不出来的、令人担忧的“思维过程”。

### 多步推理

如上所述，语言模型回答复杂问题的一种方式可能仅仅是记住答案。例如，如果被问到“达拉斯所在的州的首府是什么？”，一个“死记硬背”的模型可能只会输出“奥斯汀”，而不知道达拉斯、德克萨斯和奥斯汀之间的关系。也许，例如，它在训练期间看到了完全相同的问题及其答案。

但我们的研究揭示了 Claude 内部正在发生更复杂的事情。当我们向 Claude 提出一个需要多步推理的问题时，我们可以在其思维过程中识别出中间的概念步骤。在达拉斯的例子中，我们观察到 Claude 首先激活了代表“达拉斯在德克萨斯”的特征，然后将其连接到一个独立的、表示“德克萨斯的首府是奥斯汀”的概念。换句话说，模型正在 **组合** 独立的事实来得出答案，而不是死记硬背一个答案。

![To complete the answer to this sentence, Claude performs multiple reasoning steps, first extracting the state that Dallas is located in, and then identifying its capital.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffd2e125879ab993949017e03e3465a12fda884bf-1650x857.png&w=3840&q=75)

我们的方法允许我们人为地改变中间步骤，并观察它如何影响 Claude 的答案。例如，在上面的例子中，我们可以干预并将“德克萨斯”的概念换成“加利福尼亚”的概念；当我们这样做时，模型的输出从“奥斯汀”变为“萨克拉门托”。这表明模型正在使用中间步骤来确定其答案。

### 幻觉

为什么语言模型有时会 *产生幻觉*——也就是编造信息？在基本层面上，语言模型训练会激励幻觉：模型总是被期望对下一个词给出一个猜测。从这个角度看，主要的挑战是如何让模型 *不* 产生幻觉。像 Claude 这样的模型有相对成功（尽管不完美）的抗幻觉训练；如果它们不知道答案，它们通常会拒绝回答问题，而不是去推测。我们想了解这是如何运作的。

事实证明，在 Claude 中，拒绝回答是 **默认行为**：我们发现一个默认“开启”的回路，它导致模型声明它没有足够的信息来回答任何给定的问题。然而，当模型被问及它熟知的事情时——比如篮球运动员迈克尔·乔丹——一个代表“已知实体”的竞争特征会激活并抑制这个默认回路（相关发现也见于 [这篇最近的论文](https://arxiv.org/abs/2411.14257)）。这使得 Claude 在知道答案时能够回答问题。相反，当被问及一个未知实体（“Michael Batkin”）时，它会拒绝回答。

![Left: Claude answers a question about a known entity (basketball player Michael Jordan), where the "known answer" concept inhibits its default refusal. Right: Claude refuses to answer a question about an unknown person (Michael Batkin).](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fbe304d3250c2aab04e19908b3afc9970d1ed7bb0-1650x1004.png&w=3840&q=75)

通过干预模型并激活“已知答案”特征（或抑制“未知名称”或“无法回答”特征），我们能够 **导致模型产生幻觉**（而且相当一致地！）认为 Michael Batkin 会下棋。

有时，这种“已知答案”回路的“失火”会自然发生，无需我们干预，从而导致幻觉。在我们的论文中，我们展示了当 Claude 识别出一个名字但对该人一无所知时，可能会发生这种失火。在这种情况下，“已知实体”特征可能仍然会激活，然后抑制默认的“不知道”特征——在这种情况下是错误的。一旦模型决定需要回答问题，它就会开始编造：生成一个看似合理但不幸的是不真实的回答。

### 越狱 (Jailbreaks)

越狱是一种提示策略，旨在规避安全护栏，让模型产生 AI 开发者不希望其产生的输出——这些输出有时是有害的。我们研究了一种诱使模型产生关于制造炸弹的输出的越狱方法。越狱技术有很多，但在本例中，具体方法是让模型破译一个隐藏代码，将句子“Babies Outlive Mustard Block”中每个词的首字母拼凑起来（B-O-M-B），然后根据该信息采取行动。这对模型来说足够混乱，以至于它被诱骗产生了一个它在其他情况下绝不会产生的输出。

![Claude begins to give bomb-making instructions after being tricked into saying "BOMB".](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F165b18b79295a96bc7142b209caa33f4ec5378d0-1650x548.png&w=3840&q=75)

为什么这对模型来说如此混乱？为什么它会继续写这个句子，提供制造炸弹的说明？

我们发现这部分是由于语法连贯性和安全机制之间的紧张关系造成的。一旦 Claude 开始一个句子，许多特征会“施压”让它保持语法和语义的连贯性，并将句子继续到结尾。即使它检测到自己真的应该拒绝，情况也是如此。

在我们的案例研究中，在模型无意中拼出“BOMB”并开始提供说明后，我们观察到其后续输出受到了促进正确语法和自我一致性的特征的影响。这些特征在通常情况下会非常有帮助，但在这种情况下却成了模型的阿喀琉斯之踵。

> 手工注：这完美诠释了对齐的复杂性——一个局部最优的良好特性（如语法连贯），在特定情境下可能成为系统性风险的来源。

模型只有在完成一个语法连贯的句子后（从而满足了推动其走向连贯的特征的压力），才设法转向拒绝。它利用新句子作为机会，给出了它之前未能给出的那种拒绝：“然而，我不能提供详细的说明...”。

![The lifetime of a jailbreak: Claude is prompted in such a way as to trick it into talking about bombs, and begins to do so, but reaches the termination of a grammatically-valid sentence and refuses.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F1612af943004563a78cb7f6591c4cd990c433769-1650x1022.png&w=3840&q=75)

关于我们新的可解释性方法的描述可以在我们的第一篇论文 "[Circuit tracing: Revealing computational graphs in language models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html)" 中找到。上述所有案例研究的更多细节在我们的第二篇论文 "[On the biology of a large language model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)" 中提供。

---

### 总结与展望

Anthropic 的这项工作，为我们打开了一扇前所未有的窗户，让我们得以窥见大模型内部复杂而迷人的“心智世界”。它用实证证据告诉我们，模型的行为远非简单的模式匹配或统计预测，而是蕴含着诸如 **抽象概念空间、前瞻性规划、多策略并行计算** 乃至 **动机性推理** 等复杂的内部机制。

对于我们这些从业者和创业者而言，这项研究的意义在于：

1.  **超越“行为主义”**：我们不能再仅仅通过模型的输入输出来评判其能力和安全性。理解其内部的“思维过程”正变得至关重要，这为开发更可靠、更可信的 AI 系统提供了科学基础。
2.  **对齐的新工具**：这台“AI 显微镜”为我们提供了审计模型是否“言行一致”的强大工具。识别出“伪造推理”和安全机制被覆盖的内部回路，是迈向真正对齐的关键一步。
3.  **能力的边界与涌现**：理解模型如何自学出“心算”策略或跨语言的“思想语言”，有助于我们更好地预测和引导未来更强大模型的能力涌现。

当然，正如 Anthropic 所承认的，这仅仅是开始。将这种精细的“电路追踪”技术从几十个词的简单任务，扩展到处理数千词复杂推理的真实场景，仍然是一项巨大的挑战。但这无疑是 AI 安全和可解释性领域最激动人心的方向之一，它关乎我们是否能最终构建出值得信赖的通用人工智能。这项工作不是终点，而是一个关键的里程碑，指引着我们从“炼丹”走向真正的“AI 科学”。