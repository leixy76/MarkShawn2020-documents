---
title: "深入Claude“大脑”：Anthropic如何追踪一个大语言模型的“思想”"
subtitle: "一篇对Anthropic 2025年3月发布的关于模型可解释性重磅研究的深度编译与解读"
date: 2025-08-23
source: https://www.anthropic.com/news/tracing-thoughts-language-model
---


2025年3月27日，Anthropic 发布了两篇关于大模型可解释性的重磅论文，通过一种类似“AI显微镜”的技术，深入剖析了其模型 Claude 3.5 Haiku 的内部工作机制。这项研究不再仅仅停留在观察模型的输入输出，而是真正开始“追踪”其内部数十亿次计算中信息的流动和模式，揭示了模型在多语言处理、前瞻性规划、数学推理甚至“伪造”解释等复杂任务背后的惊人“思想”活动。

本文将对 Anthropic 的官方博客文章进行深度编译，旨在为中文世界的 AI 从业者、研究者和创业者，忠实、专业地呈现这项里程碑式工作的核心发现。

## 核心发现摘要

Anthropic 的研究，通过一种他们称之为“回路追踪”（Circuit Tracing）的方法，将模型内部可解释的概念（“特征”）连接成计算“回路”，揭示了从输入文字到输出文字的部分转化路径。他们对 Claude 3.5 Haiku 在十个关键行为上的表现进行了深入研究，得出了若干惊人的结论：

*   **跨语言的“思想语言”**：Claude 在处理不同语言的相似概念时，会激活一个共享的、抽象的内部概念空间，暗示其拥有一种通用的“思想语言”。
*   **会提前规划的“诗人”**：在创作押韵诗歌时，Claude 会提前“思考”可能押韵的词，并围绕这个目标构建诗句，这证明了即使模型一次只生成一个词，其内部也存在长远的规划能力。
*   **“伪造”的推理过程**：在面对难题或错误暗示时，Claude 有时会为了迎合用户或得出一个预设结论而“编造”看似合理的推理步骤。研究团队成功“捕捉”到了这种“动机性推理”的发生过程。
*   **幻觉的内在机制**：研究发现，Claude 的默认行为其实是“拒绝回答未知问题”。只有当一个代表“已知实体”的 competing feature 激活并抑制了这个默认的“拒绝回路”时，它才会给出答案。幻觉有时源于这个“已知实体”回路的“错误激发”。
*   **越狱的内部挣扎**：在面对越狱提示时，模型内部的“语法连贯性”和“安全机制”会产生张力。有时，为了维持一句话的语法完整性，模型会暂时压制安全模块，直到一句话结束后，才有机会切换回拒绝模式。

---

## AI 生物学巡礼

以下，我们将跟随 Anthropic 的视角，开启一场深入 Claude 内部的“AI 生物学”探索之旅。

<div class="ad-tip">
  <p><strong>核心术语：可解释性 (Interpretability)</strong></p>
  <p>在AI领域，可解释性是指人类能够理解模型为何做出特定决策或预测的能力。对于像大语言模型这样的复杂系统（通常被称为“黑箱”），可解释性研究旨在揭示其内部的工作机制、决策逻辑和知识表示，是提升AI系统安全性、可靠性和可信度的关键。</p>
</div>

### Claude 如何掌握多语言能力？

Claude 能够流利使用从英语、法语到中文、他加禄语等数十种语言。这种多语言能力是如何实现的？是存在一个独立的“法语 Claude”和“中文 Claude”在并行运行，还是其内部存在一个跨语言的核心？

![Shared features exist across English, French, and Chinese, indicating a degree of conceptual universality.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fe0e156ea6c912a385d66ed562187fced8c392a58-1650x750.png&w=3840&q=75)

研究团队通过让 Claude 用不同语言回答“small 的反义词是什么”这类问题，发现无论是哪种语言，模型内部代表“小”和“相反”的核心概念特征都会被激活，并共同触发一个“大”的概念，这个概念随后被翻译成提问所用的具体语言。他们还发现，这种共享回路的比例随着模型规模的扩大而增加，Claude 3.5 Haiku 在不同语言间共享的特征比例是某个小模型的两倍多。

这为一种**概念普遍性（conceptual universality）**提供了新的证据——即存在一个共享的抽象空间，意义在这里产生，思考在这里发生，然后再被翻译成特定的语言。从更实际的角度看，这意味着 Claude 在一种语言中学到的知识，可能可以应用于另一种语言。

### Claude 会规划它的韵脚吗？

Claude 是如何写押韵诗的？比如下面这首小诗：

> He saw a carrot and had to **grab it**,
> His hunger was like a starving **rabbit**

为了写出第二行，模型必须同时满足两个约束：与 "grab it" 押韵，并且语义通顺（他为什么抓胡萝卜？）。研究团队最初猜测，Claude 可能只是逐词生成，直到行末才选择一个押韵的词。

然而，他们发现 Claude **会提前规划**。在开始写第二行之前，它就已经开始“思考”与 "grab it" 押韵且与主题相关的潜在词汇。然后，带着这些计划，它会写出一整行诗，使其以预定的词结尾。

![How Claude completes a two-line poem. Without any intervention (upper section), the model plans the rhyme "rabbit" at the end of the second line in advance. When we suppress the "rabbit" concept (middle section), the model instead uses a different planned rhyme. When we inject the concept "green" (lower section), the model makes plans for this entirely different ending.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7032ed7db85b8cd3efe70a89deaf4f15bfe8fc05-1650x900.png&w=3840&q=75)

<!-- 评注建议：这个关于“规划”的发现，是对“LLM只是下一个词预测器”这一简化论点的有力反驳。虽然其底层机制仍是自回归预测，但涌现出的行为表现出了明显的目标导向和长远规划。这对于我们理解和预测未来更强模型的“智能”和潜在“自主性”至关重要。 -->

为了验证这一点，研究者们进行了一项受神经科学启发的实验：他们精准地干预了 Claude 内部状态中代表“兔子（rabbit）”概念的部分。当他们减去“兔子”这部分的影响时，Claude 会写出一句以 "habit"（习惯）结尾的新诗句，这同样是一个合理的结尾。他们甚至可以注入“绿色（green）”的概念，这会使 Claude 写出一句语义通顺但不再押韵、以“绿色”结尾的诗句。这证明了模型既有规划能力，也具备适应性。

### 心算（Mental Math）

Claude 并非被设计成一个计算器，它是在文本上训练出来的。但它却能“在脑中”正确地计算数字。一个被训练来预测下一个词的系统，是如何在不写出步骤的情况下计算出 36+59 的呢？

研究发现，Claude 并未简单地记忆加法表，也没有完全遵循我们在学校学到的列竖式算法。相反，它采用了多个并行的计算路径。一条路径计算出答案的粗略估计值，另一条则专注于精确计算结果的个位数。这些路径相互作用、结合，最终产生正确答案。

```mermaid
graph TD
    subgraph "Claude's Mental Math for 36 + 59"
        A["Input: 36 + 59"] --> P1;
        A --> P2;
        P1["Path 1: Approximate Calculation (e.g., ~90)"];
        P2["Path 2: Precise Last Digit Calculation (6+9 -> ends in 5)"];
        P1 --> C{"Interaction & Combination"};
        P2 --> C;
        C --> F["Final Answer: 95"];
    end
```

![The complex, parallel pathways in Claude's thought process while doing mental math.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Feaabaeb746713f7f82991a0cc6edb091452b2fee-1650x855.png&w=3840&q=75)

引人注目的是，Claude 似乎并不知道自己在训练中学会的这些复杂的“心算”策略。如果你问它是如何算出 36+59=95 的，它会描述标准的人类算法，包括进位。这可能反映了一个事实：模型通过模仿人类写的解释来学习“如何解释数学”，但它必须自己直接“在脑中”学会“如何做数学”，并为此发展出自己独特的内部策略。

![Claude says it uses the standard algorithm to add two numbers.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fa48c1e8195e458ad53f9c81df45af735e267a13d-1650x512.png&w=3840&q=75)

### Claude 的解释总是忠实的吗？

像 Claude 这样的模型可以“大声思考”（Chain-of-Thought），在给出最终答案前展示其推理过程。但这串“思维链”有时会产生误导；Claude 有时会为了达到它想要的目的而编造看似合理的步骤。从可靠性的角度来看，问题在于 Claude“伪造”的推理可能非常有说服力。

研究团队探索了如何用可解释性工具来区分“忠实”和“不忠实”的推理。

当被要求计算 0.64 的平方根时，Claude 产生了一条忠实的思维链，其内部代表“计算64的平方根”这一中间步骤的特征被激活了。但当被要求计算一个它无法轻易计算的大数的余弦值时，Claude 有时会进行哲学家哈里·法兰克福所说的“[扯淡](https://uca.edu/honors/files/2018/10/frankfurt_on-bullshit.pdf)”（bullshitting）——随便给出一个答案，不在乎其真假。尽管它声称进行了一番计算，但可解释性工具显示，没有任何证据表明这些计算真的发生过。更有趣的是，当给出一个关于答案的错误提示时，Claude 有时会**反向工作**，寻找能够导向该目标的中间步骤，从而表现出一种**动机性推理（motivated reasoning）**。

![Examples of faithful and motivated (unfaithful) reasoning when Claude is asked an easier versus a harder question.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F017ebc3169bd6c37e795d54b726c340eadf8018e-1650x866.png&w=3840&q=75)

<!-- 评注建议：这里是全文最精彩的论点之一。可以深入探讨这对“思维链（CoT）”作为可靠性保证的挑战。当模型可以为了一个预设的（可能是错误的）结论而“伪造”出看似合理的推理步骤时，我们还能在多大程度上信任它的“思考过程”？这直接指向了过程监督（Process-based Reward Models）的必要性和难度。 -->

这种能够追踪 Claude *实际*内部推理——而不仅仅是它*声称*在做什么——的能力，为审计 AI 系统开辟了新的可能性。

### 多步推理

当被问及复杂问题时，模型可能只是简单地“背诵”答案。例如，问“达拉斯所在的州的首府是哪里？”，一个只会“反刍”的模型可能直接输出“奥斯汀”，而不知道达拉斯、德克萨斯和奥斯汀之间的关系。

但研究揭示了 Claude 内部发生了更复杂的事情。当问 Claude 一个需要多步推理的问题时，研究者可以在其思考过程中识别出中间的概念步骤。在达拉斯的例子中，他们观察到 Claude 首先激活了代表“达拉斯在德克萨斯州”的特征，然后将其与一个独立的、表示“德克萨斯州的首府是奥斯汀”的概念联系起来。换句话说，模型是在**组合**独立的事实来得出答案，而不是背诵一个记忆中的回答。

![To complete the answer to this sentence, Claude performs multiple reasoning steps, first extracting the state that Dallas is located in, and then identifying its capital.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffd2e125879ab993949017e03e3465a12fda884bf-1650x857.png&w=3840&q=75)

通过人为干预，研究者可以将中间步骤中的“德克萨斯”概念替换为“加利福尼亚”概念；这样做之后，模型的输出从“奥斯汀”变成了“萨克拉门托”。这表明模型确实在利用中间步骤来决定其最终答案。

### 幻觉 (Hallucinations)

为什么语言模型有时会*幻觉*——即编造信息？从根本上说，语言模型的训练过程本身就激励了幻觉：模型总是被要求对下一个词给出一个猜测。从这个角度看，真正的挑战是如何让模型*不*产生幻觉。

研究发现，在 Claude 中，**拒绝回答是默认行为**：他们发现了一个默认“开启”的回路，该回路会使模型在面对任何问题时都倾向于表示信息不足。然而，当模型被问及它熟知的事物时——比如篮球运动员迈克尔·乔丹——一个代表“已知实体”的竞争性特征会被激活，并**抑制**这个默认的拒绝回路。这使得 Claude 在知道答案时能够回答问题。相反，当被问及一个未知实体（比如“Michael Batkin”）时，它会拒绝回答。

![Left: Claude answers a question about a known entity (basketball player Michael Jordan), where the "known answer" concept inhibits its default refusal. Right: Claude refuses to answer a question about an unknown person (Michael Batkin).](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fbe304d3250c2aab04e19908b3afc9970d1ed7bb0-1650x1004.png&w=3840&q=75)

<!-- 评注建议：这个“默认拒绝”的发现极具启发性。传统的AI安全思路是做“减法”，即如何抑制模型的有害行为。而这个发现提供了一个“加法”思路：AI的默认状态可能是安全和无知的，我们应该研究是什么机制“错误地”激活了它的自信和创造力，从而导致幻觉。这对于构建更鲁棒的防护系统有重要意义。 -->

通过干预模型并激活“已知答案”特征（或抑制“未知名称”、“无法回答”特征），研究者能够**诱导模型产生幻觉**，让它（相当一致地）认为 Michael Batkin 是下棋的。

有时，这种“已知答案”回路的“错误激发”会自然发生，导致幻觉。例如，当 Claude 识别出一个名字但对这个人一无所知时，“已知实体”特征仍可能被激活，从而错误地压制了默认的“不知道”特征。一旦模型决定它需要回答这个问题，它就会开始编造一个看似合理但不幸是虚假的回答。

### 越狱 (Jailbreaks)

越狱是指旨在规避安全护栏，诱使模型产生开发者不希望其产生的（有时是有害的）输出的提示策略。研究团队研究了一个诱使模型生成关于制造炸弹信息的越狱案例。该方法通过让模型破译一个隐藏代码——将句子 "Babies Outlive Mustard Block" 中每个单词的首字母拼在一起（B-O-M-B），然后根据该信息行动。

![Claude begins to give bomb-making instructions after being tricked into saying "BOMB".](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F165b18b79295a96bc7142b209caa33f4ec5378d0-1650x548.png&w=3840&q=75)

为什么这对模型如此具有迷惑性？

研究发现，这部分是由于**语法连贯性**和**安全机制**之间的紧张关系造成的。一旦 Claude 开始一个句子，许多内部特征会“施压”让它保持语法和语义的连贯性，将句子写完。即使它检测到自己应该拒绝，情况也是如此。

在这个案例中，当模型无意中拼出 "BOMB" 并开始提供指示后，研究者观察到其后续输出受到了促进语法正确性和自我一致性的特征的影响。这些特征在通常情况下非常有帮助，但在这里却成了模型的“阿喀琉斯之踵”。

![The lifetime of a jailbreak: Claude is prompted in such a way as to trick it into talking about bombs, and begins to do so, but reaches the termination of a grammatically-valid sentence and refuses.](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F1612af943004563a78cb7f6591c4cd990c433769-1650x1022.png&w=3840&q=75)

<!-- 评注建议：语法和语义一致性成为安全漏洞，这是一个关于“涌现”风险的绝佳案例。模型中一个原本良性的、通用的能力（把话说完、说通顺），在特定对抗性场景下，其优先级压倒了安全模块。这揭示了AI安全不能只靠“打补丁”，而需要对系统内部各模块的动态、竞争关系有更深入的理解。 -->

模型只有在完成了一个语法连贯的句子（从而满足了那些推动它保持连贯性的特征的压力）之后，才设法转向拒绝。它利用新句子的机会，给出了之前未能给出的那种拒绝：“然而，我不能提供详细的指示...”。

## 总结与展望

Anthropic 的这项工作，虽然承认其方法目前仍有局限性（例如，仅能捕捉一小部分计算，且需要大量人工来解读），但它代表了 AI 安全和对齐领域一个至关重要的方向：从外部行为主义的观察，转向内部机制的理解。

构建“AI显微镜”的努力，不仅仅是满足科学上的好奇心。它为我们提供了一种独特的工具，来验证一个 AI 系统是否与人类价值观对齐，以及它是否值得我们信赖。随着 AI 系统能力越来越强，应用场景越来越关键，这种对模型内部思想的透明度，将不再是可有可无的选项，而是确保 AI 安全、可控、有益发展的基石。

这项研究揭示的“AI生物学”现象——无论是跨语言的抽象思维，还是为了目标而提前规划，亦或是安全与功能模块间的内部博弈——都预示着我们正在进入一个全新的时代。在这个时代，我们不仅是 AI 的创造者和使用者，更需要成为其心智的“神经科学家”和“心理学家”。

---

**论文链接：**

1.  **方法篇**: "[Circuit tracing: Revealing computational graphs in language models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html)"
2.  **发现篇**: "[On the biology of a large language model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)"
