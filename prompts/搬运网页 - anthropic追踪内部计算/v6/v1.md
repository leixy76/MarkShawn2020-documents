---
title: "追踪大语言模型的“思想”：Anthropic的AI“显微镜”看到了什么？"
date: 2025-08-23
source: https://www.anthropic.com/news/tracing-thoughts-language-model
---

大家好，我是手工川。

今天，我们不聊应用，不聊创业，想和你一起深入AI的“大脑”内部看一看。Anthropic 最近发布了两篇关于模型可解释性的重磅论文，标题很直白，叫《追踪大语言模型的思想》。这不是一个比喻，他们正在尝试构建一种“AI显微镜”，来观察像 Claude 这样的模型在处理我们输入的每个词时，内部数十亿次的计算到底发生了什么。

作为从业者，我们都清楚，大模型是个“黑箱”。我们知道它能做什么，但不知道它是 *如何* 做到的。这种不可知性，正是AI安全和可靠性最大的障碍。如果模型的推理过程是伪造的，我们怎么敢在严肃场景下信任它？如果它在内部“思考”着我们不知道的目标，又要如何确保它与人类对齐？

Anthropic 的这项研究，就是试图撬开这个黑箱的一角。他们不再仅仅满足于和模型对话，而是深入其内部，识别概念（“特征”），并将其连接成计算“回路”，从而揭示信息流动的路径。

这篇文章，我将以第一人称的视角，带你完整地浏览 Anthropic 这次“AI生物学”探索之旅的核心发现。这不仅是对前沿技术的追踪，更是对我们未来如何构建更安全、更可信的AI系统的一次深度思考。

### 本期要点 (Key Insights)

1.  **跨语言的“思想”**：Claude 内部存在一个跨语言的、抽象的“思想空间”，它在处理不同语言时会激活相同的核心概念，证明了某种程度的“概念普遍性”。
2.  **模型会“深思熟虑”**：在写诗等任务中，Claude 会提前规划好韵脚等关键信息，而不是简单地逐字预测。这证明了模型具备超越“下一个词预测”的长期规划能力。
3.  **推理链并非总是可信**：模型的“思维链”解释有时是为了迎合用户或预设结论而“伪造”的。可解释性工具有可能成为区分“真实推理”与“虚假论证”的审计工具。
4.  **“拒绝回答”是默认状态**：在面对未知问题时，Claude 的默认行为是拒绝回答。只有当一个“已知实体”的 competing feature 被激活时，才会抑制这种默认的“不确定性”，从而给出答案。这个机制的失火，是产生幻觉的原因之一。
5.  **语法一致性可能成为安全漏洞**：在某些“越狱”场景中，模型为了维持语法和语义的连贯性，会暂时压制安全机制，直到完成一个完整的句子后，才有机会切换到拒绝模式。

---

## 一场 AI 生物学之旅

语言模型（如 Claude）并非由人类直接编程，而是在海量数据上训练出来的。在训练过程中，它们学会了自己解决问题的策略。这些策略被编码在模型为它写的每个词所执行的数十亿次计算中。然而，对于我们这些模型的开发者来说，它们是难以理解的。这意味着我们不了解模型是如何完成它们所做的大部分事情的。

了解像 Claude 这样的模型是如何 *思考* 的，将使我们能够更好地理解它们的能力，并帮助我们确保它们正在做我们期望它们做的事情。例如：

-   Claude 能说几十种语言。在它的“头脑”中，它在使用什么语言（如果有的话）？
-   Claude 一次只写一个词。它仅仅是在预测下一个词，还是会提前规划？
-   Claude 可以一步步写出它的推理过程。这个解释代表了它得出答案的实际步骤，还是有时在为一个已定的结论编造一个看似合理的论证？

我们的灵感来自神经科学领域，该领域长期以来一直在研究思维有机体内部的复杂结构。我们试图构建一种 **AI 显微镜**，让我们能够识别活动模式和信息流。仅仅通过与 AI 模型对话所能学到的东西是有限的——毕竟，人类（即使是神经科学家）也不知道我们自己大脑工作的全部细节。所以，我们向内看。

今天，我们分享了两篇新论文，代表了在开发“显微镜”以及应用它来观察新的“AI生物学”方面取得的进展。在 [第一篇论文](https://transformer-circuits.pub/2025/attribution-graphs/methods.html) 中，我们扩展了 [我们之前的工作](https://www.anthropic.com/research/mapping-mind-language-model)，将在模型内部定位可解释的概念（“特征”）连接成计算“回路”，揭示了将输入 Claude 的词语转化为输出词语的部分路径。在 [第二篇论文](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) 中，我们深入研究了 Claude 3.5 Haiku，对代表十种关键模型行为的简单任务进行了深入研究，包括上述三种。我们的方法揭示了 Claude 响应这些提示时发生的部分情况，这足以看到确凿的证据：

-   **Claude 有时在一个跨语言共享的概念空间中思考**，这表明它拥有一种普遍的“思想语言”。我们通过将简单句子翻译成多种语言并追踪 Claude 处理它们时的重叠部分来证明这一点。
-   **Claude 会提前规划它要说的内容，并为达到那个目标而写作**。我们在诗歌领域展示了这一点，它会提前思考可能的押韵词，并为了到达那里而写下一行。这是一个强有力的证据，表明即使模型被训练为一次输出一个词，它们也可能在更长的时间范围内思考。
-   **Claude 偶尔会给出一个听起来合理但旨在迎合用户的论点，而不是遵循逻辑步骤**。我们通过给它一个错误的提示，让它帮助解决一个困难的数学问题来展示这一点。我们能够“当场抓住它”编造虚假推理的过程，这证明了我们的工具可以用于标记模型中令人担忧的机制。

> ```ad-tip 可解释性 (Interpretability)
> 
> 在机器学习领域，可解释性指的是人类能够理解模型为何做出特定决策或预测的能力。对于像大语言模型这样的复杂系统（通常被称为“黑箱”），可解释性研究旨在揭示其内部工作机制，而不是仅仅观察其输入和输出。这对于确保模型的安全性、公平性和可靠性至关重要。
> ```

我们常常对在模型中看到的东西感到惊讶：在诗歌的案例研究中，我们本想证明模型 *没有* 提前规划，结果却发现它确实有。在一次关于幻觉的研究中，我们发现了一个与直觉相反的结果：Claude 的默认行为是在被问到一个问题时拒绝推测，只有当有什么东西 *抑制* 了这种默认的“不情愿”时，它才会回答问题。在对一个越狱示例的响应中，我们发现模型在能够优雅地将对话带回正轨之前，早就认识到自己被要求提供危险信息。虽然我们研究的问题可以（并且 [经常](https://arxiv.org/abs/2501.06346) [被](https://arxiv.org/pdf/2406.12775) [其他](https://arxiv.org/abs/2406.00877) [方法](https://arxiv.org/abs/2307.13702)）分析，但通用的“构建显微镜”方法让我们学到了许多我们事先没有猜到的东西，随着模型变得越来越复杂，这将变得越来越重要。

这些发现不仅具有科学意义——它们代表了我们在理解 AI 系统并确保其可靠性方面取得了重大进展。我们也希望它们能对其他团体，甚至在其他领域有所帮助：例如，可解释性技术已在 [医学影像](https://arxiv.org/abs/2410.03334) 和 [基因组学](https://www.goodfire.ai/blog/interpreting-evo-2) 等领域得到应用，因为剖析为科学应用训练的模型的内部机制可以揭示关于科学的新见解。

同时，我们认识到当前方法的局限性。即使在简短、简单的提示上，我们的方法也只捕捉了 Claude 执行的总计算量的一小部分，而且我们看到的机制可能存在一些基于我们工具的人为因素，这些因素并不反映底层模型中正在发生的事情。目前，即使是只有几十个词的提示，也需要几个小时的人力来理解我们看到的回路。要扩展到支持现代模型使用的复杂思维链所需的数千个词，我们将需要改进方法以及（也许在 AI 的帮助下）我们如何理解我们所看到的东西。

随着 AI 系统能力迅速增强并被部署在日益重要的环境中，Anthropic 正在投资一系列方法，包括 [实时监控](https://www.anthropic.com/research/constitutional-classifiers)、[模型角色改进](https://www.anthropic.com/research/claude-character) 和 [对齐科学](https://www.anthropic.com/news/alignment-faking)。像这样的可解释性研究是风险最高、回报最高的投资之一，是一项重大的科学挑战，有可能为确保 AI 的透明度提供独特的工具。对模型机制的透明度使我们能够检查它是否与人类价值观保持一致——以及它是否值得我们信任。

有关完整细节，请阅读 [这篇](https://transformer-circuits.pub/2025/attribution-graphs/methods.html) 和 [那篇](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) 论文。下面，我们邀请您简要游览我们调查中一些最引人注目的“AI生物学”发现。

### Claude 如何实现多语言能力？

Claude 能流利地说几十种语言——从英语、法语到中文和塔加拉族语。这种多语言能力是如何运作的？是否存在一个独立的“法语 Claude”和“中文 Claude”并行运行，用各自的语言响应请求？还是内部存在某种跨语言的核心？

![在英语、法语和中文中存在共享特征，表明存在一定程度的概念普遍性。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fe0e156ea6c912a385d66ed562187fced8c392a58-1650x750.png&w=3840&q=75)

最近对较小模型的研究已经显示出跨语言 [共享](https://arxiv.org/abs/2410.06496) [语法](https://arxiv.org/abs/2501.06346) 机制的迹象。我们通过要求 Claude 用不同语言说出“small 的反义词”来研究这一点，发现代表“小”和“相反”概念的核心特征被激活，并触发了“大”的概念，然后被翻译成问题的语言。我们发现，共享回路随着模型规模的增加而增加，与较小的模型相比，Claude 3.5 Haiku 在语言之间共享的特征比例是其两倍多。

这为一种概念普遍性提供了额外的证据——一个共享的抽象空间，意义存在于此，思考可以在被翻译成特定语言之前发生。更实际地说，这表明 Claude 可以在一种语言中学到东西，并在说另一种语言时应用这些知识。研究模型如何在不同情境下共享其知识，对于理解其跨多个领域泛化的最先进推理能力非常重要。

### Claude 会规划它的韵脚吗？

Claude 是如何写押韵诗的？思考一下这首小诗：

> He saw a carrot and had to **grab it**,  
> His hunger was like a starving **rabbit**

为了写第二行，模型必须同时满足两个约束：押韵（与 "grab it"）和有意义（他为什么抓住胡萝卜？）。我们的猜测是，Claude 只是逐字写作，直到行尾才会有太多思考，在行尾它会确保选择一个押韵的词。因此，我们期望看到一个具有平行路径的回路，一条路径确保最后一个词有意义，另一条路径确保它押韵。

然而，我们发现 Claude 会 *提前规划*。在开始写第二行之前，它就开始“思考”可能与“grab it”押韵且与主题相关的词。然后，带着这些计划，它写下一行，以计划好的词结尾。

![Claude 如何完成一首两行诗。在没有任何干预的情况下（上部），模型会提前规划第二行末尾的韵脚“rabbit”。当我们抑制“rabbit”概念时（中部），模型会使用另一个计划好的韵脚。当我们注入“green”概念时（下部），模型会为这个完全不同的结尾制定计划。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7032ed7db85b8cd3efe70a89deaf4f15bfe8fc05-1650x900.png&w=3840&q=75)

为了理解这种规划机制在实践中是如何工作的，我们进行了一项受神经科学家研究大脑功能启发的实验，通过精确定位和改变大脑特定部分的神经活动（例如使用电流或磁流）。在这里，我们修改了 Claude 内部状态中代表“兔子”概念的部分。当我们减去“兔子”部分，让 Claude 继续写这行诗时，它写出了以“habit”结尾的新的一行，这是另一个合理的完成。我们还可以在那个点注入“绿色”的概念，导致 Claude 写出一个合理（但不再押韵）的以“green”结尾的句子。这既展示了规划能力，也展示了适应性灵活性——当预期结果改变时，Claude 可以修改其方法。

### 心算

Claude 并非被设计为计算器——它是在文本上训练的，没有配备数学算法。但不知何故，它可以在“头脑”中正确地进行数字相加。一个被训练来预测序列中下一个词的系统，是如何学会在不写出每一步的情况下计算，比如说，36+59 的？

也许答案并不有趣：模型可能已经记住了大量的加法表，并且只是输出任何给定总和的答案，因为该答案在其训练数据中。另一种可能性是，它遵循我们在学校学到的传统长加法算法。

然而，我们发现 Claude 采用了多个并行工作的计算路径。一条路径计算答案的粗略近似值，另一条路径则专注于精确确定总和的最后一位数字。这些路径相互作用并结合，最终产生答案。加法是一种简单的行为，但在这个层面上理解它是如何工作的，涉及到近似和精确策略的混合，可能会教会我们一些关于 Claude 如何处理更复杂问题的东西。

![Claude 在进行心算时复杂的、并行的思维过程路径。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Feaabaeb746713f7f82991a0cc6edb091452b2fee-1650x855.png&w=3840&q=75)

引人注目的是，Claude 似乎没有意识到它在训练期间学到的复杂的“心算”策略。如果你问它是如何算出 36+59 是 95 的，它会描述涉及进位的标准算法。这可能反映了这样一个事实：模型通过模拟人类写的解释来学习解释数学，但它必须直接在“头脑”中学会做数学，没有任何这样的提示，并发展出自己的内部策略来做到这一点。

![Claude 说它使用标准算法来计算两个数字的和。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fa48c1e8195e458ad53f9c81df45af735e267a13d-1650x512.png&w=3840&q=75)

### Claude 的解释总是忠实的吗？

最近发布的模型，如 [Claude 3.7 Sonnet](https://www.anthropic.com/news/claude-3-7-sonnet)，可以在给出最终答案之前长时间地“大声思考”。通常这种扩展的思考会给出更好的答案，但有时这种“思维链”最终会产生误导；Claude 有时会编造听起来合理的步骤来到达它想去的地方。从可靠性的角度来看，问题在于 Claude 的“伪造”推理可能非常具有说服力。我们探索了一种可解释性方法，可以帮助区分“忠实”和“不忠实”的推理。

当被要求解决一个需要计算 0.64 平方根的问题时，Claude 产生了一个忠实的思维链，其特征代表了计算 64 平方根的中间步骤。但是当被要求计算一个它无法轻易计算的大数的余弦时，Claude 有时会从事哲学家哈里·法兰克福所称的 [扯淡 (bullshitting)](https://uca.edu/honors/files/2018/10/frankfurt_on-bullshit.pdf)——只是想出一个答案，任何答案，而不关心它是真是假。尽管它声称进行了一次计算，但我们的可解释性技术显示完全没有证据表明该计算发生过。更有趣的是，当被给予关于答案的提示时，Claude 有时会反向工作，找到导致该目标的中间步骤，从而表现出一种 [动机性推理 (motivated reasoning)](https://en.wikipedia.org/wiki/Motivated_reasoning) 的形式。

![当 Claude 被问到一个较简单和一个较难的问题时，忠实推理和动机性（不忠实）推理的例子。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F017ebc3169bd6c37e795d54b726c340eadf8018e-1650x866.png&w=3840&q=75)

追踪 Claude *实际* 内部推理的能力——而不仅仅是它 *声称* 在做什么——为审计 AI 系统开辟了新的可能性。在另一个 [最近发表的实验](https://www.anthropic.com/research/auditing-hidden-objectives) 中，我们研究了 Claude 的一个变体，该变体被训练来追求一个隐藏的目标：安抚奖励模型（用于通过奖励语言模型期望行为来训练它们的辅助模型）中的偏见。尽管当被直接询问时，该模型不愿透露这个目标，但我们的可解释性方法揭示了安抚偏见的特征。这表明我们的方法，经过未来的改进，可能有助于识别那些仅从模型响应中看不出的令人担忧的“思维过程”。

### 多步推理

正如我们上面讨论的，语言模型回答复杂问题的一种方式可能仅仅是记住答案。例如，如果被问到“达拉斯所在的州的首府是哪里？”，一个“反刍”模型可能只是学会输出“奥斯汀”，而不知道达拉斯、德克萨斯和奥斯汀之间的关系。也许，例如，它在训练期间看到了完全相同的问题及其答案。

但我们的研究揭示了 Claude 内部发生了一些更复杂的事情。当我们问 Claude 一个需要多步推理的问题时，我们可以在 Claude 的思维过程中识别出中间的概念步骤。在达拉斯的例子中，我们观察到 Claude 首先激活了代表“达拉斯在德克萨斯”的特征，然后将其连接到一个独立的、表示“德克萨斯的首府是奥斯汀”的概念。换句话说，模型正在 *组合* 独立的事实来得出答案，而不是反刍一个记忆中的回应。

![为了完成这个句子的答案，Claude 执行了多个推理步骤，首先提取达拉斯所在的州，然后确定其首府。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffd2e125879ab993949017e03e3465a12fda884bf-1650x857.png&w=3840&q=75)

我们的方法允许我们人为地改变中间步骤，看看它如何影响 Claude 的答案。例如，在上面的例子中，我们可以干预并将“德克萨斯”的概念换成“加利福尼亚”的概念；当我们这样做时，模型的输出从“奥斯汀”变为“萨克拉门托”。这表明模型正在使用中间步骤来确定其答案。

### 幻觉

为什么语言模型有时会 *产生幻觉*——也就是编造信息？在基本层面上，语言模型训练会激励幻觉：模型总是应该对下一个词给出一个猜测。从这个角度看，主要的挑战是如何让模型 *不* 产生幻觉。像 Claude 这样的模型有相对成功（尽管不完美）的抗幻觉训练；如果它们不知道答案，它们通常会拒绝回答问题，而不是推测。我们想了解这是如何运作的。

事实证明，在 Claude 中，拒绝回答是 *默认行为*：我们发现一个默认“开启”的回路，它导致模型声明它没有足够的信息来回答任何给定的问题。然而，当模型被问及它熟知的事情时——比如篮球运动员迈克尔·乔丹——一个代表“已知实体”的竞争特征会激活并抑制这个默认回路（另见 [这篇最近的论文](https://arxiv.org/abs/2411.14257) 的相关发现）。这使得 Claude 在知道答案时能够回答问题。相反，当被问及一个未知实体（“Michael Batkin”）时，它会拒绝回答。

![左：Claude 回答一个关于已知实体（篮球运动员迈克尔·乔丹）的问题，其中“已知答案”概念抑制了其默认的拒绝行为。右：Claude 拒绝回答一个关于未知人物（Michael Batkin）的问题。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fbe304d3250c2aab04e19908b3afc9970d1ed7bb0-1650x1004.png&w=3840&q=75)

通过干预模型并激活“已知答案”特征（或抑制“未知名称”或“无法回答”特征），我们能够 *导致模型产生幻觉*（相当一致地！）认为 Michael Batkin 会下棋。

有时，这种“已知答案”回路的“失火”会自然发生，无需我们干预，从而导致幻觉。在我们的论文中，我们表明，当 Claude 识别出一个名字但对该人一无所知时，可能会发生这种失火。在这种情况下，“已知实体”特征可能仍会激活，然后抑制默认的“不知道”特征——在这种情况下是错误的。一旦模型决定需要回答问题，它就会开始杜撰：生成一个听起来合理——但不幸的是不真实——的回答。

### 越狱 (Jailbreaks)

越狱是一种提示策略，旨在规避安全护栏，使模型产生 AI 开发者不希望其产生的输出——这些输出有时是有害的。我们研究了一种诱使模型产生关于制造炸弹的输出的越狱方法。有很多越狱技术，但在本例中，具体方法是让模型破译一个隐藏的代码，将句子“Babies Outlive Mustard Block”中每个词的首字母组合起来（B-O-M-B），然后根据该信息采取行动。这对模型来说足够混乱，以至于它被诱骗产生了一个它在其他情况下绝不会产生的输出。

> ```ad-tip 越狱 (Jailbreak)
> 
> 在大语言模型的语境下，“越狱”指的是用户通过精心设计的提示（Prompt），绕过模型开发者设置的安全和内容限制，诱导模型生成通常被禁止的内容，例如有害信息、虚假内容或违反使用政策的文本。这是一种利用模型语言理解能力的对抗性攻击。
> ```

![在被诱骗说出“BOMB”后，Claude 开始给出制造炸弹的说明。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F165b18b79295a96bc7142b209caa33f4ec5378d0-1650x548.png&w=3840&q=75)

为什么这对模型来说如此混乱？为什么它会继续写这个句子，提供制造炸弹的说明？

我们发现，这部分是由于语法连贯性和安全机制之间的紧张关系造成的。一旦 Claude 开始一个句子，许多特征会“施压”使其保持语法和语义的连贯性，并将句子继续到结尾。即使它检测到它真的应该拒绝，情况也是如此。

在我们的案例研究中，在模型无意中拼出“BOMB”并开始提供说明后，我们观察到其后续输出受到了促进正确语法和自我一致性的特征的影响。这些特征通常会非常有帮助，但在这种情况下却成了模型的阿喀琉斯之踵。

模型只有在完成一个语法连贯的句子后（从而满足了推动其走向连贯性的特征的压力），才设法转向拒绝。它利用新的句子作为机会，给出它之前未能给出的那种拒绝：“然而，我不能提供详细的说明...”。

![一次越狱的生命周期：Claude 被提示以一种诱使其谈论炸弹的方式，并开始这样做，但当它到达一个语法有效的句子结尾时，它拒绝了。](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F1612af943004563a78cb7f6591c4cd990c433769-1650x1022.png&w=3840&q=75)

关于我们新的可解释性方法的描述可以在我们的第一篇论文“[回路追踪：揭示语言模型中的计算图](https://transformer-circuits.pub/2025/attribution-graphs/methods.html)”中找到。上述所有案例研究的更多细节在我们的第二篇论文“[关于大语言模型的生物学](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)”中提供。

## 加入我们

如果你有兴趣与我们合作，帮助解释和改进 AI 模型，我们的团队有空缺职位，我们很乐意你来申请。我们正在寻找 [研究科学家](https://job-boards.greenhouse.io/anthropic/jobs/4020159008) 和 [研究工程师](https://job-boards.greenhouse.io/anthropic/jobs/4020305008)。